\chapter{28 апреля}

\section{Минимизация квадратичной функции}

Квадратичные функции имеют вид:
\[f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x} + C\]
, где \(A\) --- симметричная матрица, также являющейся матрицей Гессе \(\mathbf{H}\) для \(f(x)\). При этом \(\nabla f(x) = Ax + b\).

Если матрица \(A\) невырождена, то в силу необходимого условия экстремума \(f(x)\) имеет единственную стационарную точку \(x^* = - A^{ - 1} b\), что получается при \(\nabla f(x) = 0\)

С другой стороны, \(x^*\) является точкой наименьшего значения \(f(x)\) тогда и только тогда, когда квадратичная функция \(\ev{Ax, x}\) положительно определена.

Не умаляя общности\footnote{Сдвинув систему координат, если это требуется.} допустим, что \(x^* = \vec{0}\), т.е. \(b = 0\). Тогда
\begin{equation}
    f(x) = \frac{1}{2} \ev{Ax, x}, \quad x \in E_n
    \label{f}
\end{equation}

Т.к. мы рассматриваем положительно определенную \(A\), то квадратичная функция \(\frac{1}{2} \ev{Ax, x}\) неотрицательна в \(E_n\) и достигает наименьшего значения \(0\) в единственной точке \(x^* = 0\).

\subsection{Метод градиентного спуска}
Для \(f(x)\) из \eqref{f} \(\nabla f(x) = Ax\) в точке \(x\). Пусть начальное приближение \(x^0 \neq 0\), тогда антиградиент \(w^1 = - \nabla f(x^0) = - Ax^0\), и в общем виде:
\[x^k = x^{k - 1} + \alpha_k w^k, \quad k\in N\]

\begin{equation}
    x^1 = x^0 + \alpha_1 w^1 = x^0 - \alpha_1 A x^0 = (I\footnote{Единичная матрица} - \alpha_1 A) x_0
    \label{x1}
\end{equation}

Из \eqref{x1} следует, что точку минимума квадратичной функции можно достичь за одну итерацию, если \(x^0\) --- собственный вектор матрицы \(A\). Кроме того, т.к. \(Ax^0 = \lambda_j x^0\), где \(x_0\) --- собственный вектор \(A\), а \(\lambda_j\) --- соответствующее собственное значение, \((A - \lambda_j I)x^0 = 0\). Тогда если \(\alpha_1 = \frac{1}{\lambda_j}\), то:
\[x^1 = \left( I - \frac{1}{\lambda_j} A \right) x^0 = - \frac{1}{\lambda_j} (A - \lambda_j I) x^0 = 0\]
, т.е. \(x^1 = x^* = 0\)

Геометрическая интерпретация: в двумерном случае \(f(x)\) есть эллиптический параболоид с центром в начале координат. Метод градиентного спуска приведет в точку \((0, 0)\) за одну итерацию, если начальная точка выбрана на одной из осей эллипсов, т.е. радиус-вектор точки является собственным вектором \(A\).

В частности, если \(A = \lambda I\), то каждый ненулевой вектор является собственным. Таким образом, для такой матрицы минимум достигается за одну итерацию при любом выборе \(x^0\).

Квадратичную форму вида \eqref{f} можно привести к так называемому каноническому виду, т.е. к виду с единичной матрицей. Рассмотрим, как это сделать.

Применим ортогональное преобразование к форме \(f\) и получим \(f_1\), тогда:
\[f_1 (\xi) = \sum_{j = 1}^n \lambda_j \xi_j^2\]
, где \(\xi = \begin{pmatrix} \xi_1 & \dots & \xi_n \end{pmatrix}, \lambda_j\) --- положительные собственные значения \(A, n\) --- количество оных.

Изменим масштаб переменных заменой
\[\eta_j = \sqrt{\lambda_j} \xi_j\]
, тогда
\[f_2(\eta) = \sum_{j = 1}^n \eta_j^2\]

После этих замен минимизация функции выполняется за одну итерацию при любом выборе \(x^0\) по выкладкам выше.

Казалось бы, этот подход очень помогает в минимизации функций. Однако для данных преобразований требуется решить сложную задачу вычисления собственных значений матрицы. В силу этого обычно используются более совершенные методы. Однако этот метод быстро сходится даже для функций овражного характера.

\subsection{Метод градиентного спуска с константным шагом}

Пусть \(\alpha_k = \alpha = \const\) на всех итерациях.

На \(k\)-той итерации:
\begin{equation}
    x^k = x^{k - 1} + \alpha w^k = (I - \alpha A) x^{k - 1}
    \label{переход}
\end{equation}

Оценивать сходимость релаксационной последовательности \(\{x^k\}\) можно с помощью \textbf{теоремы о неподвижной точке}. Согласно данной теореме \(\{x^k\}\) сходится к неподвижной точке \(x^*\) отображения \(f(x)\), если данное отображение является \textbf{сжимающим}, то есть для него выполняется условие Липшица:
\begin{equation}
    |f(x) - f(y)| \leq q |x - y|
    \label{условие Липшица}
\end{equation}
, где \(q = \const < 1\)

В \eqref{переход} \(I - \alpha A\) есть сжимающее отображение, если норма этого отображения \( < 1\). Для симметричных матриц норма равна спектральной норме, которая в свою очередь равна \(\max_{i} |\lambda_i|\), где \(\lambda_i\) --- собственные числа данной матрицы.

Таким образом, для сходимости релаксационной последовательности достаточно выполнения
\[q(\alpha) = ||I - \alpha A|| < 1\]

Упорядочим собственные значения матрицы \(A\) как:
\[0 < \lambda_1 < \dots < \lambda_n\]
Матрица \(I - \alpha A\) имеет собственные значения вида \(1 - \alpha \lambda_j\), а следовательно:
\[1 - \alpha \lambda_n < 1 - \alpha \lambda_{n - 1} < \dots < 1 - \alpha \lambda_1 < 1\]
и условие \(||I - \alpha A|| < 1\) равносильно условию:
\[\begin{cases}
        1 - \alpha \lambda_n > - 1 \\
        1 - \alpha \lambda_n < 1
    \end{cases}
    \Rightarrow \alpha \in \left( 0, \frac{2}{\lambda_n} \right)\]

Кроме того, из теоремы о неподвижной точки следует, что для \(\{x^k\}\) выполнено \(|x^k - x^*| \leq q^k |x^0 - x^*|\). Тогда для ускорения сходимости релаксационной последовательности \(q\) должно быть как можно меньше.

\(q(\alpha)\) минимально, если \(1 - \alpha \lambda_n\) и \(1 - \alpha \lambda_1\) совпадают по абсолютны по знаку:
\[ - (1 - \alpha \lambda_1) = 1 - \alpha \lambda_n\]
\[\alpha^* = \frac{2}{\lambda_1 + \lambda_n} \leq \frac{2}{\lambda_n}\]

При \(\alpha = \alpha^*\):
\begin{equation}
    q^* = q(\alpha^*) = \frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1} = \frac{\cond A - 1}{\cond A + 1}
    \label{q*}
\end{equation}

Если \(\cond A \gg 1\), функция имеет овражную стуктуру и метод сходится медленно.

Если \(A = I\), то все собственные значения \(1\), \(\cond A = 1\) и \(q^* = 0\), а следовательно минимум достигается за одну итерацию \(\forall x^0\).

Если \(\alpha \notin \left( 0, \frac{2}{\lambda_n} \right)\), \(\{x^k\}\) не релаксационная, а метод расходится или зацикливается.

\subsection{Минимизация с использованием исчерпывающего спуска}

Этот метод нарушает условие \(\alpha \in \left( 0, \frac{2}{\lambda_n} \right)\).

Исчерпывающий спуск на \(k\)-той итерации ищет стационарную точку \(\varphi_k(\alpha) = f(x^{k - 1} + \alpha w^k)\).
\[\varphi_k(\alpha) = \frac{1}{2} \ev{A(x^{k - 1} + \alpha w^k), x^{k - 1} + \alpha w^k} = f(x^{k - 1}) + \alpha\ev{Ax^{k - 1}, w^k} + \frac{\alpha^2}{2}\ev{A w^k, w^k}\]
Эта функция имеет положительный коэффициент при старшей степени, следовательно она имеет единственную стационарную точку
\begin{equation}
    \alpha_k = - \frac{\ev{Ax^{k - 1}, w^k}}{\ev{Aw^k, w^k}} = \frac{|w^k|^2}{\ev{Aw^k, w^k}}
    \label{alphak}
\end{equation}

Из \eqref{alphak}:
\begin{itemize}
    \item \(\alpha_k = \frac{1}{\lambda_n}\), если \(x^{k - 1}\) --- собственный вектор \(A\) с собственным значением \(\lambda_n\)
    \item \(\alpha_k = \frac{1}{\lambda_1}\), если \(x^{k - 1}\) --- собственный вектор \(A\) с собственным значением \(\lambda_1\)
\end{itemize}

Если \(\cond A = \frac{\lambda_n}{\lambda_1} > 2\), то \(\alpha_k \in \left( 0, \frac{2}{\lambda_n} \right)\) нарушается при \(\alpha_k = \frac{1}{\lambda_1}\).

Для квадратичной функции метод наискорейшего спуска эквивалентен градиентному методу с исчерпывающим спуском, т.к. квадратичная функция является строго выпуклой.

\subsection{Метод сопряженных направлений}

Правило перехода:
\begin{equation}
    x^k = x^{k - 1} + \alpha_k p^k, \quad k \in N
    \label{правило перехода}
\end{equation}
, где \(\alpha_k\) --- шаг, \(p^k\) --- вектор спуска.

Если \(A\) --- симметричная положительная определенная матрица, то \(\ev{x, y}_A = \ev{Ax, y}\) --- скалярное произведение в \(E_n\). Тогда задача приведения квадратичной формы к каноническому виду сводится к выбору ортонормированного базиса в евклидовом пространстве со скалярным произведением \(\ev{x, y}_A\)

Плюсы подхода:
\begin{itemize}
    \item Позволяет упростить вид квадратичной функции.
    \item Не требует нахождения собственных значений, позволяет использовать ортогонализацию.
\end{itemize}

\begin{definition}
    Если \(p^1 \neq 0, p^2 \neq 0, \ev{Ap^1, p^2} = 0\), то такие вектора называются \textbf{сопряженными} относительно положительно определенной матрицы \(A\) или  \textbf{\(A\)-ортогональными}. Направления, определенные \(p^1\) и \(p^2\), также называются сопряженными.
\end{definition}

Рассмотрим \(A\)-ортогональный базис \(p^j\). В этом базисе \(f\) имеет канонический вид:
\[f_1(\xi) = \lambda_1 \xi_1^2  + \dots + \lambda_n \xi_n^2\]
, где \(\xi = \begin{pmatrix} \xi_1 & \dots & \xi_n \end{pmatrix}, \lambda_j = \frac{1}{2} ||p^j||^2_A\)

Функция \(f_1(\xi)\) --- сепарабельна\footnote{Представима в виде \(\sum_{i = 1}^n f_i(\xi_i)\)}, а следовательно:
\begin{itemize}
    \item Исчерпывающий спуск в направлении \(p^j\) минимизирует одно из слагаемых такой функции.
    \item Последовательность из \(n\) исчерпывающих спусков в направлениях \(p^1 \dots p^n\) минимизирует все слагаемые, следовательно минимизирует искомую функцию.
\end{itemize}

Рассмотрим первый спуск.
\[x^1 = x^0 - \xi_1^1 p^1\]
, где \(\xi_1^0\) --- первая координата \(x^0\) в ортогональном базисе \(p^j\):
\[\xi_1^0 = \frac{\ev{Ax^0, p^1}}{\ev{Ap^1, p^1}}\]

Можем заметить, что рассмотренная формула и \eqref{переход} отличаются только в знаке.

\begin{theorem}
    Точка минимума квадратичной функции \(f(x) = \frac{1}{2} \ev{Ax, x}\) с положительно определенной матрицей \(A\) достигается не более чем за \(n\) итераций спуска, если направления спуска задаются векторами \(p^k \in E_n\), сопряженными относительно матрицы \(A\), а параметры \(\alpha_k\), определяющие шаг спуска в \eqref{переход}, вычисляются по формуле исчерпывающего спуска
    \begin{equation}
        \alpha_k = - \frac{\ev{A x^{k - 1}, p^k}}{\ev{A p^k, p^k}}
        \label{формула исчерпывающего спуска}
    \end{equation}
\end{theorem}

\begin{remark}
    Если \(p^j\) и \(- p^j\) в точке \(x^{j - 1}\) не определяют направление спуска, то \(\ev{Ax^{j - 1}, p^j} = 0\), следовательно \(\alpha_j = 0\) и спуск в направлении \(p^j\) не производится, число итераций \(< n\).
\end{remark}

Координаты \(x^0\) в \(A\)-ортогональном базисе можно выразить через скалярное произведение
\begin{equation}
    x^0 = \sum_{i = 1}^n \frac{\ev{Ax^0, p^i}}{\ev{Ap^i, p^i}} p^i
    \label{x0 в базисе}
\end{equation}

Если \(x^*\) --- точка минимума квадратичной формы с положительно определенной матрицей \(A\), то:
\[x^0 - x^* = \sum_{i = 1}^n \frac{\ev{A(x^0 - x^*), p^i}}{\ev{Ap^i, p^i}} p^i = \sum_{i = 1}^n \frac{\nabla f(x^0), p^i}{\ev{Ap^i, p^i}} p^i\]
\begin{equation}
    x^* = x^0 - \sum_{i = 1}^n \frac{\nabla f(x^0), p^i}{\ev{Ap^i, p^i}} p^i
    \label{x**}
\end{equation}

\[\ev{Ax^0, p^i} p^i = p^i \ev{p^i, Ax^0} = p^i (p^i)\tran Ax^0\]
И тогда \eqref{x0 в базисе} можно записать как:
\[x^0 = \sum_{i = 1}^n \frac{p^i (p^i)\tran }{\ev{Ap^i, p^i}} Ax^0\]
Т.к. это верно \(\forall x^0\), то рассматриваемый оператор тождественный:
\[\sum_{i = 1}^n \frac{p^i (p^i)\tran }{\ev{Ap^i, p^i}} A = \mathbf{1}\]
Следовательно:
\[A^{ - 1} = \sum_{i = 1}^n \frac{p^i (p^i)\tran }{\ev{Ap^i, p^i}}\]
Таким образом, по \(p^i\) можно построить \(A^{ - 1}\) и \eqref{x**} можно переписать как:
\begin{align*}
    x^*
     & = x^0 - \sum_{i = 1}^n \frac{\nabla f(x^0), p^i}{\ev{Ap^i, p^i}} p^i        \\
     & = x^0 - \sum_{i = 1}^n \frac{p^i (p^i)\tran }{\ev{Ap^i, p^i}} \nabla f(x^0) \\
     & = x^0 - A^{ - 1} \nabla f(x^0)                                              \\
\end{align*}

В итоге, выполнение всех \(n\) итераций исчерпывающего спуска есть один спуск вида
\[x^* = x^0 - A^{ - 1} \nabla f(x^0)\]

Основа метода сопряженных направлений --- использование векторов \(p^j\).

Различие в способах построения сопряженных векторов порождает несколько вариантов метода сопряженных направлений.

Теперь рассмотрим минимизацию функции
\[f(x) = \frac{1}{2} \ev{Ax, x} + \ev{b, x}\]

Мы можем ортогонализовать любой базис, но более эффективно исходить из системы антиградиентов, ортогонализуя базис в процессе спуска.

Выберем приближение \(x^0 \in E_n\). Первая итерация:
\[w^1 = - \nabla f(x^0) = - Ax^0 - b\]
\[p^1 : = w^1\]
Если \(|p^1| \neq 0\), то \(p^1\) --- направление спуска, иначе \(x^0 = x^*\) и процесс окончен.

Производим исчерпывающий спуск в направлении \(p^1\) по формуле \eqref{alphak}:
\[\alpha_1 = \frac{|w^1|^2}{\ev{Aw^1, w^1}} = \frac{|p^1|^2}{\ev{Ap^1, p^1}}\]
\[x^1 = x^0 + \alpha_1 p^1\]

Вторая итерация:
\[w^2 = - Ax^1 - b\]
Если \(|w^2| = 0\), то\(x^0 = x^*\) и процесс окончен, иначе проведем ортогонализацию \(p^1\) и \(w^2\) относительно скалярного произведения \(\ev{x, y}_A\):
\begin{equation}
    p^2 = w^2 - \frac{\ev{Ap^1, w^2}}{\ev{Ap^1, p^1}} p^1
    \label{p2}
\end{equation}

\(p^2\) --- направление спуска из \(x^1\):
\[\ev{\nabla f(x^1), p^2} = - \ev{w^2, \beta_1 p^1 + w^2} = - \ev{w^2, w^2} = - |w^2|^2 < 0\]
, где \(\beta_1 = - \frac{\ev{Ap^1, w^2}}{\ev{Ap^1, p^1}}\)

Дальнейшие итерации аналогичны.

Уточнения:
\begin{enumerate}
    \item Каждый \(w^k\) ортогонален не только предпоследнему направлению спуска \(p^{k - 1}\), но и всем \(p^i, i < k - 1\)

          \textcolor{red}{Доказательство опущено.}

    \item Антиградиент \(w^k\) сопряжен с \(\forall p^i\):
          \[\alpha_i \ev{Ap^i, w^k} = \ev{w^i - w^{i + 1}, w^k} = \ev{w^i, w^k} - \ev{w^{i + 1}, w^k} = 0\]
\end{enumerate}

Из этих уточнений следует, что процесс ортогонализации можно записать в виде:
\[p^k = w^k - \sum_{i = 1}^{k - 1} \frac{\ev{Ap^i, w^k}}{\ev{Ap^i, p^i}} p^i\]
Следовательно:
\[p^k = w^k - \frac{\ev{Ap^{k - 1}, w^k}}{\ev{A p^{k - 1}, p^{k - 1}}} p^{k - 1}\]
Это главный выигрыш в использовании системы антиградиентов.

Общая схема метода:
\begin{itemize}
    \item \(k = 1\):

          Выбираем \(x^0, w^1 = - Ax^0 - b, p^1 = w^1, \alpha_1 = \frac{|p^1|^2}{\ev{Ap^1, p^1}}, x^1 = x^0 + \alpha_1 p^1\)

    \item \(k > 1\):

          \begin{equation}
              \begin{cases}
                  w^k = - Ax^{k - 1} - b                                                         \\
                  p^k = w^k - \frac{\ev{Ap^{k - 1}, w^k}}{\ev{A p^{k - 1}, p^{k - 1}}} p^{k - 1} \\
                  x^k = x^{k - 1} + \alpha_k p^k
              \end{cases}
              \label{fdsa}
          \end{equation}
\end{itemize}

\(\alpha_k\) определяется из условия исчерпывающего спуска, например:
\[\alpha_k = \frac{\ev{w^k, p^k}}{\ev{Ap^k, p^k}}\]

Метод сопряженных направлений также можно использовать для неквадратичных функций, если заменить \(A\) на матрицу Гессе.

% Модификации:
% \textcolor{red}{Опущено}

\unfinished
